WebXR-Based Imitation Learning Data Collection Infrastructure for the Unitree G1 Humanoid: A Comprehensive Implementation ArchitectureThe collection of high-quality, large-scale demonstration data remains the primary bottleneck in training generalized Vision-Language-Action (VLA) models and imitation learning (IL) policies for embodied agents. Traditional teleoperation paradigms often rely on leader-follower hardware setups, exoskeleton suits, or two-dimensional monitor interfaces. These conventional methods suffer from severe spatial mismatch, high latency, and restricted scalability due to the absolute necessity of physical robot availability. Furthermore, physical data collection is fundamentally constrained by hardware duty cycles, battery life, and the persistent risk of catastrophic hardware failure during complex manipulation tasks. Transitioning the teleoperation interface entirely into a web-based Extended Reality (WebXR) environment offers a highly scalable, hardware-agnostic alternative that democratizes the data collection process.This document provides an exhaustive, critical analysis and an actionable implementation roadmap for developing a fully browser-based Virtual Reality data collection application using the Meta Quest 3 headset. The target embodiment is the Unitree G1 humanoid robot equipped with Dex 3.1 three-fingered dexterous hands. The application is explicitly designed to simulate the robot's kinematics, control limits, and environmental physics natively within the headset's browser. This architecture allows operators to record complete, physically validated episode trajectories—comprising proprioceptive states, actions, and precise object poses—that can subsequently be rendered offline into rich visual datasets for imitation learning pipelines. The methodology detailed herein addresses the visual gap, the kinematic gap, and the dynamic gap inherent in teleoperation, providing a definitive blueprint for an automated coding agent to construct the system.Hardware Topology and Kinematic ConstraintsTo engineer an effective virtual teleoperation environment, the digital twin must rigorously and mercilessly adhere to the kinematic and dynamic constraints of the physical hardware. Failure to replicate these constraints accurately in the virtual environment invariably leads to domain gap anomalies. In such scenarios, human operators provide demonstrations that the physical robot fundamentally cannot execute due to singularity configurations, joint velocity limits, mechanical self-collisions, or payload restrictions. The resulting imitation learning datasets become polluted with mathematically valid but physically impossible trajectories, rendering the trained policies useless in real-world deployment.Unitree G1 Humanoid Morphological PropertiesThe Unitree G1 represents a highly articulated platform designed for complex loco-manipulation tasks. Depending on the specific configuration tier (ranging from the 23 Degrees of Freedom standard version to the 29 or 31 Degrees of Freedom EDU and Ultimate versions equipped with dexterous hands), the upper body features a comprehensive joint space. The arm structure provides seven degrees of freedom per arm, enabling human-like reachability and the necessary redundancy for sophisticated obstacle avoidance and complex manipulation maneuvers.The movement ranges for the upper body and legs are tightly constrained by the mechanical design. These hard limits must be mathematically enforced within the WebXR Inverse Kinematics (IK) solver to prevent the generation of invalid action labels during the data collection process.Joint HierarchyPitch Limits (Radians / Degrees)Roll Limits (Radians / Degrees)Yaw Limits (Radians / Degrees)Flexion LimitsShoulder$\pm 154^\circ$Left: $-91^\circ$ to $+129^\circ$Right: $-129^\circ$ to $+91^\circ$$\pm 150^\circ$Not ApplicableElbowNot ApplicableNot ApplicableNot Applicable$0^\circ$ to $165^\circ$Wrist$\pm 92.5^\circ$Not Applicable$\pm 92.5^\circ$Not ApplicableHip$-2.53$ to $+2.87$ rad$-0.52$ to $+2.96$ rad$\pm 2.75$ radNot ApplicableKneeNot ApplicableNot ApplicableNot Applicable$-0.08$ to $+2.87$ radBeyond the spatial limits, the physical payload capacity of the basic G1 arm is restricted to two kilograms, extending to three kilograms in the EDU versions, while the standing payload supports up to twelve kilograms. The WebXR environment must visually and physically penalize the operator if an attempt is made to manipulate simulated objects whose mass exceeds these thresholds. Enforcing payload limits in the virtual physics engine ensures the collected imitation data respects real-world actuator torque limitations.Dex 3.1 Dexterous Hand Actuation and DynamicsThe Dex 3.1 end-effector is a force-controlled, three-fingered dexterous hand with a total of seven active degrees of freedom, designed for precision grasping and robust manipulation. A profound understanding of its mechanical topology is paramount for accurately mapping the twenty-five-point human hand tracking data provided by the Meta Quest 3 into the seven-dimensional action space of the robot.The mechanical transmission comprises six direct-drive motors and one gear-driven motor, distributing power and control across the digits. The specific limits of these joints represent the boundaries of the robot's interaction capabilities and must be hard-coded into the virtual model.Digit ClassificationJoint NomenclatureRange of MotionActuation MechanismThumb (3 DoF)Joint 0$-60^\circ$ to $+60^\circ$Direct DriveThumbJoint 1$-35^\circ$ to $+60^\circ$Direct DriveThumbJoint 2$0^\circ$ to $+100^\circ$Gear DrivenIndex Finger (2 DoF)Joint 0$0^\circ$ to $+90^\circ$Direct DriveIndex FingerJoint 1$0^\circ$ to $+100^\circ$Direct DriveMiddle Finger (2 DoF)Joint 0$0^\circ$ to $+90^\circ$Direct DriveMiddle FingerJoint 1$0^\circ$ to $+100^\circ$Direct DriveA highly critical operational constraint for the Dex 3.1 relates to its maximum load condition. Extensive hardware testing indicates that when grasping a five-centimeter round hard object at room temperature, the maximum weight capacity is strictly five hundred grams, while the maximum power output peaks at four hundred watts for a maximum duration of three seconds. The fingertip positioning accuracy is rated at $\pm 2$ millimeters. In the context of building a virtual data collector, the five-hundred-gram limit directly dictates the maximum frictional and normal forces the virtual fingers can exert within the physics engine. Permitting forces exceeding this threshold in the simulation would result in imitation learning policies that attempt mathematically viable but physically impossible power grasps when deployed to the real hardware.WebXR Egocentric Interface and Spatial AlignmentThe Meta Quest 3 represents a paradigm shift in accessible spatial computing, featuring advanced inside-out tracking and sophisticated articulated hand tracking algorithms. This hardware exposes the poses of twenty-five distinct skeletal joints per hand through the standardized WebXR Device API. By actively requesting the hand-tracking optional feature descriptor during the WebXR session initialization, the browser context gains access to high-fidelity, low-latency spatial data representing the human operator's intent.To collect highly usable imitation learning data, the spatial and psychological alignment between the human operator and the virtual robot must be exact. The interface is required to render the Unitree G1 robot from a strictly egocentric, first-person perspective.Immersive Presence and the Visual-Proprioceptive LoopWhen the operator looks downward within the headset, the visual feedback must display the G1's torso and lower extremities rather than an empty void. More importantly, when the operator raises their hands into their field of view, the system must render the precise Computer-Aided Design (CAD) meshes of the Dex 3.1 hands, expressly avoiding any rendering of the human user's actual hands or generic digital representations of human anatomy.This visual substitution provides crucial psychological and proprioceptive feedback, effectively tricking the human brain into adopting the embodiment of the robot. If the operator visually perceives the physical limits, kinematic constraints, and geometric volume of the Dex 3.1 fingers, they naturally and subconsciously adjust their reaching and grasping strategies to match the robot's specific morphology. This phenomenon significantly reduces the kinematic gap between human intuitive intent and rigid robot execution, resulting in demonstration trajectories that are natively compatible with the target hardware. Without this direct visual replacement, operators tend to perform complex five-finger manipulations that the three-fingered Dex 3.1 simply cannot replicate.Coordinate Frame Synchronization and CalibrationThe WebXR camera matrix must be rigidly and mathematically attached to the head link of the G1 Universal Robot Description Format (URDF) model. However, human anatomical proportions, such as interpupillary distance, arm span, and overall height, invariably differ from the specific dimensions of the Unitree G1. Therefore, the application architecture must mandate an initial calibration sequence prior to any data collection.During this calibration, the human operator's spatial origin must be translationally and proportionally scaled. This scaling ensures that when the human operator fully extends their physical arm, it maps perfectly to the G1's maximum reach workspace, which is rated at zero to two meters vertically. This transformation matrix is critical; without it, an operator might physically reach for a virtual object only to find the virtual robot arm fully extended and locked at a singularity before the object is reached, entirely ruining the data collection episode. The calibration routine computes an anatomical scaling factor that dynamically adjusts the inverse kinematics target vectors, ensuring the user's physical workspace is perfectly inscribed within the robot's mechanical workspace.Kinematic Retargeting and Inverse KinematicsThe core computational challenge in this application involves translating fluid, highly articulated human motion into constrained robotic joint motion in real-time, while strictly enforcing the robot's physical limitations. This complex translation requires a sophisticated, three-tiered algorithmic approach consisting of Topological Hand Retargeting, Damped Inverse Kinematics, and Impedance Control.Topological Mapping: 25-Point Human to 7-DoF Robot HandThe WebXR API continuously streams the positional and rotational poses for the human thumb, index, middle, ring, and pinky fingers, encompassing metacarpal, phalanx proximal, phalanx intermediate, and phalanx distal joints. Conversely, the Dex 3.1 is mechanically limited to a thumb, an index finger, and a middle finger. The retargeting algorithm must map this rich twenty-five-point human configuration down to the seven-degree-of-freedom robot configuration without losing the operator's grasping intent.A mathematically rigorous approach in modern telemanipulation is the continuous optimization of fingertip positions. Let $\mathcal{F}_{\text{human}}$ represent the forward kinematics of the human hand as tracked by the WebXR system, and let $\mathcal{F}_{\text{robot}}$ represent the forward kinematics of the Dex 3.1 hand derived from its URDF model. The fundamental objective is to compute the specific robot joint angles $\theta_{\text{robot}} \in \mathbb{R}^7$ that minimize the Euclidean distance between the corresponding functional fingertips in Cartesian space:$$\min_{\theta_{\text{robot}}} \sum_{i \in \{\text{thumb, index, middle}\}} \left\| \mathcal{F}_{\text{human, i}}(x_{\text{human}}) - \mathcal{F}_{\text{robot, i}}(\theta_{\text{robot}}) \right\|^2$$This optimization is strictly subject to the physical joint limits defined previously: $\theta_{\text{min}} \le \theta_{\text{robot}} \le \theta_{\text{max}}$.Because the Dex 3.1 hardware entirely lacks ring and pinky fingers, the human's ring and pinky joint data streams are algorithmically discarded during the optimization process, treating the human operator as if they were operating a three-fingered claw. To implement this intensive optimization within the confines of a web browser, existing Python-based libraries such as dex-retargeting  must be ported directly to TypeScript or, for optimal performance, compiled to WebAssembly (WASM). Running this optimization in WASM ensures the retargeting executes within the stringent eleven-millisecond frame budget required to maintain a ninety-Hertz refresh rate in the Quest 3, preventing simulator sickness.Damped Least Squares Inverse Kinematics for the 7-DoF ArmSimultaneous to the hand retargeting, the Quest 3 provides the six-degree-of-freedom pose, encompassing both spatial position and quaternion orientation, of the human wrist. The computational goal here is to calculate the seven respective joint angles of the Unitree G1 arm required to reach this desired pose. Because a seven-degree-of-freedom arm is kinematically redundant—meaning it possesses one more degree of freedom than the six required for absolute rigid body positioning in three-dimensional space—there are an infinite number of mathematical solutions to reach a given point.Standard Jacobian pseudo-inverse methods, commonly used in simpler robotics, often fail catastrophically near kinematic singularities. As the arm approaches full extension, the required joint velocities computed by the pseudo-inverse approach infinity, an acceleration profile that a real motor cannot possibly execute. To solve this instability in a JavaScript environment, the application must utilize a Damped Least Squares (DLS) Inverse Kinematics solver, such as the architecture provided by the closed-chain-ik-js library. The DLS method modifies the standard pseudo-inverse calculation by introducing a damping factor $\lambda$:$$\Delta \theta = J^T (J J^T + \lambda^2 I)^{-1} \Delta x$$In this formulation, $J$ represents the Jacobian matrix of the robot arm, $\Delta x$ signifies the desired change in the end-effector pose dictated by the human operator, and $\Delta \theta$ represents the resulting required change in the robot's joint angles. The critical damping factor $\lambda$ artificially penalizes high joint velocities, preventing erratic mathematical solutions when the human operator fully extends their arm or moves rapidly through a singular configuration. This guarantees that the joint trajectories recorded for the imitation learning dataset remain smooth, continuous, and physically executable by the Unitree G1's actual motor controllers.Impedance Control and Virtual Physical DampingA critical requirement identified for this implementation is the necessity for the virtual robot to follow the human operator's hands in a "slowly damped" manner, explicitly acknowledging that the physical robot cannot match the instantaneous acceleration of human biological movement. If a human operator rapidly swings their arm to catch a falling object, the Unitree G1's maximum joint power output of four hundred watts  cannot instantaneously match that sheer acceleration.Mitigating the Actuator Velocity GapTo model this hardware limitation accurately, the WebXR application must architecturally decouple the raw human tracker position from the target setpoint fed into the robot's inverse kinematics solver. Directly feeding the unfiltered human hand pose into the IK solver results in teleportation-like movements in the virtual robot arm, which, if recorded as an action trajectory, would command the real robot to exert infinite torque, triggering safety faults and ruining the imitation learning policy. Instead, the system must implement a sophisticated virtual impedance control model, effectively a computational spring-damper system, bridging the human intent and the robot's execution.Virtual Spring-Damper Mathematical FormulationLet $x_{\text{human}}$ represent the real-time position of the human hand tracked in VR, and $x_{\text{target}}$ represent the smoothed setpoint that is actually fed to the robot's IK solver. The dynamic behavior of this setpoint is governed by a second-order differential equation simulating physical mass, damping, and stiffness:$$M \ddot{x}_{\text{target}} + C \dot{x}_{\text{target}} + K (x_{\text{target}} - x_{\text{human}}) = 0$$In this equation, $M$ represents the virtual mass of the robot arm, $C$ represents the damping coefficient, and $K$ represents the spring stiffness. By meticulously tuning the parameters $C$ and $K$ to match the actual proportional-derivative gains of the Unitree G1's low-level motor controllers, the virtual robot arm will intentionally lag behind the user's physical hand. This mathematical filter smooths out rapid, jerky human twitches, biological tremors, and tracking noise, strictly enforcing a maximum velocity and acceleration limit that mirrors the physical hardware.This architecture creates a highly realistic and pedagogically valuable teleoperation scenario. The operator will visually perceive their virtual Dex 3.1 hands "trailing" their physical hands if they move with excessive speed. Because the operator is forced to look at the robot hands to manipulate objects, they will naturally and subconsciously slow down their physical movements to keep the virtual hands aligned with their biological proprioception. This psychological feedback loop results in smooth, dynamically feasible, and highly deliberate trajectories, which are universally considered the gold standard for training robust imitation learning policies.Deterministic Physics Simulation in the BrowserA foundational pillar of reliable imitation learning data collection is the immutability of the physical interaction. As accurately assessed in the system requirements, physics cannot simply be recalculated offline, as minute differences in simulation stepping will drastically affect the outcome of a grasp. If an operator carefully pinches a cube in VR, the exact points of contact, the distribution of friction, and the prevention of slip define the fundamental success of the manipulation task.The Necessity of Client-Side Physics ComputationIf the system only records the robot's joint angles and attempts to replay those angles later in an offline physics simulator such as MuJoCo or NVIDIA Isaac Sim, the "butterfly effect" of numerical integration takes hold. Microscopic differences in floating-point math handling between the browser and the offline engine, disparities in collision detection algorithms, or variations in integration step sizes will cause the offline object to slip out of the robot's hand, entirely invalidating the human demonstration. A dataset filled with failed grasps is toxic to a Vision-Language-Action model. Therefore, the ground-truth physical interactions must be computed client-side, within the headset.Engine Selection: Rapier Cross-Platform DeterminismTo solve this critical synchronization issue, the browser must utilize a fully deterministic physics engine. Extensive evaluation indicates that the optimal choice for WebXR robotics simulation is the Rapier physics engine, compiled to the browser via WebAssembly. Legacy physics engines such as Ammo.js or Cannon.js either suffer from severe performance bottlenecks in complex multi-joint scenarios or lack strict determinism. Rapier, conversely, offers guaranteed cross-platform determinism provided the target platforms comply with the IEEE 754-2008 floating-point standard.This deterministic guarantee means that if the physics step is fed the exact same inputs—specifically the robot joint positions or torques at a given tick—it will produce the exact same physical outcome and object poses, regardless of the underlying CPU architecture. By running Rapier within the Quest 3 browser, the application computes and solidifies the ground-truth physics interactions at the moment of demonstration.During the data collection phase, the application must record not only the robot's proprioceptive state, including joint angles and velocities, but also the six-degree-of-freedom poses of all dynamic objects in the scene, such as the cubes, balls, and baskets, at every single frame. By exporting the exact trajectories of the environment objects alongside the robot states, researchers can completely bypass the volatile need for an offline physics engine during the visual rendering phase of training data generation. The offline pipeline simply ingests the recorded poses and renders the visual frames in a purely kinematic, forced-pose manner. This architecture guarantees that the visual data perfectly and infallibly matches the successful grasp that the user experienced and verified in VR.Imitation Learning Task ScenariosThe virtual interface must validate the system's efficacy through specific manipulation scenarios. These scenarios serve as the benchmark for stress-testing the kinematic retargeting, the stability of the Rapier physics integration, and the high-bandwidth data recording capabilities of the browser. The scenarios are chosen for their ability to elicit distinct types of grasping primitives and spatial reasoning from the human operator.Precision Manipulation: Colored Cube StackingThe first scenario involves a virtual table containing three identically sized cubes colored red, green, and blue. The operator's objective is to grasp and stack the cubes vertically in a specific, predetermined sequence.This task requires extraordinarily high-fidelity contact modeling within the Rapier physics engine. The Dex 3.1 fingertips must utilize highly tuned friction and restitution parameters. The coefficient of friction ($\mu$) must be set sufficiently high to allow the three-fingered hand to execute a successful "pinch grasp" on the flat surfaces of the cubes without them sliding out of the grip, accurately simulating the rubberized texture of the physical hand's contact pads.From an imitation learning perspective, the stacking task provides immense value. It teaches the downstream VLA policy critical spatial reasoning, sequential multi-step planning, and millimeter-level precision alignment. Furthermore, the policy must learn the subtle mechanics of releasing a grasp gracefully; opening the fingers too rapidly or with lateral force will knock over the underlying structure, a failure mode the human operator will naturally learn to avoid during data collection.Dynamic Interception: Ball Sorting and Power GraspingThe second scenario features a table populated with two designated target baskets and six spherical objects, equally divided into three red and three yellow balls. The objective is to visually identify and sort the balls into their respective color-coded baskets.Simulating spheres in rigid body dynamics introduces severe computational complexities due to continuous rolling constraints and the minimal point-contact area they share with the robot's flat fingers. To successfully manipulate the spheres, the operator using the virtual Dex 3.1 must perform a "power grasp," enveloping the ball comprehensively with the thumb, index, and middle fingers, rather than relying on a delicate pinch grasp.Crucially, Rapier's Continuous Collision Detection (CCD) algorithms must be enabled specifically for the spheres. Without CCD, rapid movements by the operator could cause the spheres to mathematically clip through the thin colliders of the virtual baskets or phase through the robot's fingers between physics frames. The IL value of this sorting task is substantial; it trains the policy in visual object categorization, dynamic interception of moving or rolling objects, and gross motor movement traversing a much wider operational workspace than the highly localized stacking task.High-Bandwidth Data Pipeline and Storage ArchitectureTraining a robust imitation learning policy requires immense datasets, often consisting of hundreds or thousands of individual episodes. Each episode contains highly synchronized pairs of observations and actions recorded at a fixed, high frequency, typically thirty or fifty Hertz. Managing this volume of data within a standalone VR headset browser presents a formidable software engineering challenge.Trajectory Serialization and RLDS FormattingTo ensure immediate compatibility with state-of-the-art training frameworks such as Hugging Face's LeRobot or Google's Reinforcement Learning Datasets (RLDS), the WebXR application must serialize the collected data into a strict, standardized schema.Key IdentifierData StructureDescription of Payloadobservation.statefloat32The actual joint positions of the G1 (15 lower body, 14 upper body joints).observation.dex_statefloat32The actual joint positions of the two Dex 3.1 hands ($7 \times 2$).actionfloat32The target joint commands generated by the impedance control and IK solver.env_state.object_posesfloat32[N*7]Position (XYZ) and Quaternion (WXYZ) for $N$ dynamic objects in the scene.timestampfloat64Monotonic Unix time ensuring exact frame synchronization.episode_idstringUnique UUID identifier for the demonstration trajectory.Visual data, such as simulated images from the head or wrist cameras, is deliberately and strategically omitted from the WebXR recording phase. The application exclusively saves the mathematical poses of the environment objects. The computationally heavy process of rendering multi-view RGB and Depth images is delegated entirely to an offline headless renderer post-collection.In-Browser Memory Management via IndexedDBRecording a sixty-second episode at fifty Hertz generates thousands of large floating-point arrays. Attempting to store this entire payload persistently in the browser's volatile memory or standard ArrayBuffer risks triggering out-of-memory crashes in the Quest 3 browser tab, resulting in total data loss. Utilizing standard localStorage is categorically impossible due to its strict five-megabyte hard limit, which accommodates only strings.The only viable architectural solution for high-bandwidth browser storage is IndexedDB. IndexedDB provides an asynchronous, high-capacity, NoSQL storage environment capable of utilizing up to fifty percent of the headset's available physical disk space, allowing for hours of continuous data collection.The application architecture must implement a multi-threaded streaming buffer system. During the active episode, the main physics loop pushes state and action arrays into a temporary memory buffer. Simultaneously, a dedicated Web Worker asynchronously flushes this buffer into IndexedDB using transactional writes. This architectural separation ensures the main WebXR render thread, which must maintain a strict ninety-Hertz cadence, is never blocked by database I/O operations. Upon episode completion, the IndexedDB records are queried, packaged into a comprehensive JSON payload, and prepared for export.Network Export and TransmissionOnce safely stored in IndexedDB, the data must be exported from the standalone headset to the researcher's primary workstation. The architecture utilizes a local network streaming approach via WebRTC or WebSockets. At the conclusion of a successful episode, the headset pushes the aggregated IndexedDB payload over the WebSocket connection to a Python server hosted on the local machine. This server immediately ingests the stream, formats it according to the LeRobot dataset specifications, and saves it directly to an HDF5 file. This automated, wireless pipeline entirely eliminates the friction of manual file transfers via USB.Offline Sensor Rendering and Domain RandomizationThe final phase of the imitation learning pipeline occurs entirely outside the headset. The raw JSON or HDF5 files containing the kinematic trajectories of the robot and the objects are processed to generate the massive visual datasets required by the Vision-Language-Action models.Kinematic Replay in MuJoCo and Isaac SimThe offline rendering script utilizes a high-fidelity simulator such as NVIDIA Isaac Sim or MuJoCo. Crucially, the offline script must explicitly disable the physics engine solver within the simulator. Instead of allowing the simulator to recalculate forces and collisions, the script forces the absolute joint states of the Unitree G1 and the Cartesian poses of all environment objects to exactly match the mathematical states recorded in the WebXR JSON trajectory on a frame-by-frame basis.This forced kinematic replay guarantees that the offline visualization perfectly mirrors the physical outcome achieved by the human operator in VR, completely bypassing any physics-based non-determinism or simulation instability.Exteroceptive Sensor GenerationWith the scene perfectly recreating the demonstration mechanically, the simulator is utilized purely as a rendering engine. Synthetic RGB and Depth camera feeds are generated from the virtual sensors mounted on the G1's head and wrists, precisely matching the hardware specifications of the physical robot's sensor suite. During this rendering process, sophisticated domain randomization techniques are heavily applied. The system alters the ambient lighting conditions, changes the procedural textures of the tables and objects, and modifies background elements across different rendering passes. This strategic randomization allows a single human VR demonstration to generate dozens of varied visual trajectories, artificially multiplying the size of the dataset and ensuring the final imitation learning policy is highly robust to varying real-world lighting and environments.Agent Implementation Master PlanTo build this highly complex, multi-threaded system, an automated coding agent or engineering team must follow a rigorous, phased implementation plan. This master plan ensures that foundational kinematic stability is achieved before complex physics and high-bandwidth data logging are introduced.The first phase focuses on environment scaffolding and asset ingestion. The agent must initialize a React-Three-Fiber application heavily integrated with the WebXR API, establishing the foundational 3D canvas. Concurrently, the @dimforge/rapier3d WebAssembly library must be implemented to handle all spatial mathematics and collisions. The agent will parse the URDF files for both the Unitree G1 and the Dex 3.1 hands using urdf-loader, programmatically mapping the extracted GLTF nodes to Rapier rigid bodies and articulated joints. Strict attention must be paid to applying the exact joint limits to the physics constraints. Finally, the egocentric XR camera rig is established, mapping the headset's spatial position directly to the G1's head link, and a scaling UI is implemented to calibrate the user's biological arm length to the robot's mechanical workspace.The second phase tackles the mathematical core: retargeting and inverse kinematics. The agent must interface with the XRHand API to continuously extract the twenty-five joint poses. The topological retargeting algorithm must be implemented in JavaScript, mapping the human thumb, index, and middle fingertips to the respective Dex 3.1 fingertips while discarding extraneous finger data. Following this, a Damped Least Squares solver, derived from libraries like closed-chain-ik-js, is integrated to control the arm. The solver tree is configured from the G1's shoulder joint down to the wrist, setting the retargeted hand root as the continuous target.The third phase bridges the dynamic gap by implementing impedance control and physical simulation. The agent must construct the virtual spring-damper logic layer. The raw XR controller pose updates the theoretical spring target, while the IK solver calculates angles to reach the spring's current state, preventing the robot from instantaneously snapping to the human's position and enforcing physical velocity limits. Accurate collision shapes, such as convex hulls and capsules, are assigned to the Dex 3.1 fingers and the environment objects, with friction coefficients meticulously tuned to mimic the physical robot's grasp. The agent must guarantee deterministic replay by ensuring that the Rapier step function runs at a fixed time step, independent of the XR display's variable refresh rate.The fourth phase involves scenario scripting and the data pipeline. The agent will script the initialization state machines for both the precision stacking and dynamic sorting tasks, ensuring initial object positions are slightly randomized to introduce necessary data variance. The IndexedDB database is instantiated, preferably utilizing a wrapper like dexie.js for transactional reliability. A dedicated Web Worker is written to handle the asynchronous serialization of the state, action, and object pose arrays at the target recording frequency without bottlenecking the render thread.The final phase finalizes the export mechanism and validates the offline pipeline. The agent will build a lightweight Python WebSocket server intended to run on the local host machine, and implement the transmission logic in the VR UI to push the IndexedDB trajectory payload over the network. The Python server immediately formats the JSON payload into the highly structured LeRobot HDF5 format. Finally, the agent provides the offline validation script for MuJoCo or Isaac Sim, detailing the specific API calls required to ingest the recorded JSON trajectory, disable the internal physics engine, force the robot joints to the recorded poses, and render the synthetic camera feeds with domain randomization. This systematic execution plan guarantees a robust, artifact-free dataset ready for advanced imitation learning.